

## 1. 数据库设计

### 1.1 用户表 user

![image-20221028141958998](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210281420343.png)

### 1.2 帖子表 discuss_post

![image-20221028142548808](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210281425105.png)

> 为什么要将帖子评论数量放在帖子表？

如果不放在帖子表，那么在查询帖子的评论数量时就需要先在评论表中查出 post_id 为当前帖子的评论数量，这样在评论表中扫描查询肯定是比直接从帖子表中进行等值查询性能差的。



### 1.3 评论表 comment

![image-20230224164904684](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202302241649050.png)



### 1.4 消息表 message

![image-20221028144604816](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210281446446.png)

注意：

- 消息表既保存用户之间的私信，也保存系统消息通知；

若保存的是用户之间的私信：

- from_id：发信息的用户 id；
- to_id：收信息的用户 id；
- conversation_id：格式规定为：[参与私信的两个用户 id 中的最小值] _ [参与私信的两个用户 id 中的最大值]。
  - 永远把较小的 id 放在前面的规定保证了私信的唯一性。例如 1_2 和 2_1 是同一个私信。
- content：私信的内容，就是普通的 text 文本格式；



若保存的是系统消息通知：

- from_id：固定为 1，表示系统管理员；
- to_id：要通知的用户 id；
- conversation_id：由于系统通知分为三大类，所以有三种不同的值，每种值对应 Kafka 的 Topic，应为系统通知是采用 Kafka 完成的，具体实现在后面。
  - follow：关注通知
  - like：点赞通知
  - comment：评论通知
- content：JSON 字符串，具体内容根据通知的类型而定：
  - follow 类型：{"entityType": 通知类型，3 表示关注，"entityId"：被通知的实体 id(对于关注来说是用户 id)，"userId": 触发通知的用户 id}
  - like 类型：{"entityType": 通知类型，2 表示点赞，"entityId": 被通知的实体 id(对于点赞来说是帖子 id)，"postId": 被点赞的帖子 id，"userId": 触发点赞的用户 id}
  - comment 类型：{"entityType": 通知类型，1 表示评论，"entityId"：被评论的实体 id(对于评论来说是帖子 id，或者被评论的评论 id)，"postId": 被评论的帖子 id，"userId": 触发评论的用户 id}



### 1.5 登录凭证表 login_ticket

该表最后被废弃，使用 Redis 代替了。

![image-20221028162221130](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210281622257.png)

凭证相当与 session，只是用了数据库表来存这个凭证信息。

在登录时将用户的信息存入 login_ticket 表中，在请求前从数据库中获取凭证，若获取成功且凭证有效说明用户已登录。



## 2. 业务开发

### 2.1 首页

首页需要的数据有帖子信息（标题，时间等）、帖子作者信息（用户的头像、用户名）。

所以只需要查询出所有的帖子，得到帖子后，再根据 user_id 查出用户信息，将它们一对一的封装到一个 Map 类型的 List 中，返回给前端即可。

为了减小数据库和浏览器的压力，需要分页返回数据。

> 分页功能的设计

自定义了一个分页的类，表示分页时所需的信息：

```java
public class Page {

    // 当前页码，默认1
    private int current = 1;

    // 每页显示的数量，默认10
    private int limit = 10;

    //数据总数，用于计算总页面数量
    private int rows;

    // 查询路径，某页的 url（用于复用分页的连接）
    private String path;

    /**
     * 获取当前页的起始行
     * @return 当前页的起始行
     */
    public int getOffset() {
        // current * limit - limit
        return (current - 1) * limit;
    }

    /**
     * 获取总页数
     * @return 总页数
     */
    public int getTotal() {
        // rows / limit [+1]
        if (rows % limit == 0)
            return rows / limit;
        else return rows/ limit + 1;
    }

    /**
     * 获取起始页码（页数较多时，只显示当前页号的前两个页号和两个页号）
     * @return 起始页码
     */
    public int getFrom() {
        int from = current - 2;
        return from < 1 ? 1 : from;
    }

    /**
     * 获取最后页码（页数较多时，只显示当前页号的前两个页号和两个页号）
     * @return 最后页码
     */
    public int getTo() {
        int to = current + 2;
        int total = getTotal();
        return to > total ? total : to;
    }

    public void setCurrent(int current) {
        if (current >= 1)
            this.current = current;
    }

    public void setLimit(int limit) {
        if (limit >= 1 && limit <= 100)
            this.limit = limit;
    }

    public void setRows(int rows) {
        if (rows >= 0)
            this.rows = rows;
    }
    
}
```

分页对象信息在前后端互传：

- 初始时后端会根据 Page 对象的默认 `current` 和 `limit` 查询出对应条数的数据传给前端；

- 前端可以根据 Page 对象获取页的总数、当前页的起始位置等信息，当点击别的页时，会将 Page 对象的 `current` 改成对应的页号，然后通过 `path` 重新访问后端，将新的 Page 传给后端，后端再获取新 Page 中的页号，重新查询数据返回；



### 2.2 注册功能

注册功能不需要返回什么数据，若注册成功则跳转到成功页面，给一个激活账号的连接，注册失败则需要将原因提示给用户。

若能注册成功：

1. 需要随机为用户生成一个盐，用于对密码加密，数据库中存储的都是加密后的密码。
2. 随机生成一个激活码，发送激活邮件，直接使用 SpringMail。



### 2.3 登录功能（重点）

因为登录成功后，需要将登录信息保存起来，方便用户进行以后的请求，所以需要考虑下面的问题。

分布式 session 问题：

传统的做法是直接查询 user 表获取 user 对象，然后存入 session 中，之后就可以根据 session 获取用户信息了。

这样的方案在分布式部署时会出现什么问题呢？

分布式部署意味着有多台服务器，而 session 是存储在服务器上的，那么如果我下次请求的服务器和我初始时请求的服务器不一样，就会因另一台服务器没有保存我的 session 而判定我为未登录状态。

![image-20221028171918119](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210281719174.png)

如何解决呢？

- 可以同步服务器的 session，但是这样不久浪费服务器资源了吗？
- 那要不，使用 cookie？cookie 以明文形式存储在客户端，不安全。

我们初步的解决方案是建一个登录凭证表，把用户信息保存到表中，有一个 ticket 字段，存放到 cookie 中，因为只是存放了一串随机字符串到 cookie，其他隐私信息都在表中，需要通过 ticket 访问表才能获取。

但是这样岂不是要额外建一张表来存放凭证信息，也有点浪费资源啊，而且每次都要查询数据库，效率也不高。



所以最终的解决方案是使用 **Redis**：

- 存储的 key 为 ticket，value 为字符串即可，因为要将登录凭证 LoginTicket 对象序列化后再存入 Redis；
- 同样，还是把 ticket 存放到 cookie 中；
- 由于 Redis 的性能高，而且设置了凭证的过期时间，过期的键会自动从 Redis 中删除，也不浪费资源；



现在在登录成功后，会生成一个带有用户信息和过期时间的登录凭证 LoginTicket 对象，保存到 Redis 中。

接着还要把生成的凭证 ticket 存入到客户端的 cookie 中，同样也要设置相同的过期时间。

之后客户端向服务器请求的时候就会带上 cookie。



那么我们从哪儿获取 cookie 呢？当然时要在用户请求页面前获取，这样才能在页面显示前把用户的状态查询出来。

我们通过 **拦截器** 来实现，使用 Spring 自带的拦截器，实现 HandlerInterceptor 后，可以重写 3 个方法：

- preHandle：在目标请求方法执行前执行；
- postHandle：在目标请求方法执行之后，视图未返回前执行；
- afterCompletion：在整个流程执行完毕后执行（DispatcherServlet 完全处理完请求后）；

最后在 WebMvcConfig（需实现 WebMvcConfigurer）的 addInterceptors 方法中添加该拦截器，即可实现拦截功能。

根据我们的业务需求，我们需要在请求前查询出用户的登录状态。

所以需要在 preHandle 中获取凭证信息，通过请求 request 获取到 cookie 后，再根据 cookie 从 Redis 汇总查询出 LoginTicket 对象，登录凭证中即有用户的 id，那么就能获取到用户的所有信息了。

那我们获取到用户信息后，为了后面的操作中可以方便的获取用户信息，怎么样保存这个用户信息呢？

一个好的方法是使用 **ThreadLocal**，**它可以在同一线程中很方便的获取用户信息，不需要频繁的传递对象**。

所以在 preHandle 中获取到用户信息后，将用户信息存入 ThreadLocal，后续在任何地方都可以从 ThreadLocal 中获取到当前用户的信息。

除此之外，当用户存入 ThreadLocal 后，我们还在 postHandle 中通过 ThreadLocal 获取用户传给前端，方便前端判断当前用户是否已具有登录凭证。

最后，当整个流程执行完毕后，记得在 afterCompletion 中清理掉 ThreadLocal，否则可能发生内存泄漏（GC 自动清理没有清理掉）。

拦截器代码如下：

```java
/**
 * @desc: 登录凭证拦截器，用于在请求前获取凭证
 * 若获取成功且凭证有效说明用户已登录，将用户存入 ThreadLocal 中，
 * 然后在生成视图之前将 ThreadLocal 中的当前用户传给视图层，
 * 视图层根据 loginUser 是否为空来判断用户是否已登录。
 * @author: AruNi_Lu
 * @date: 2022/10/11
 */
@Component
public class LoginTicketInterceptor implements HandlerInterceptor {

    private final Logger logger = LoggerFactory.getLogger(LoginTicketInterceptor.class);

    @Autowired
    private UserService userService;

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
        // 从 Cookie 中获取凭证
        String ticket = CookieUtil.getValue(request, "ticket");
        if (ticket != null) {
            // 查询凭证
            LoginTicket loginTicket = userService.findLoginTicket(ticket);
            // 检查凭证是否有效
            if (loginTicket != null && loginTicket.getStatus() == 0 && loginTicket.getExpired().after(new Date())) {
                // 根据凭证查询用户
                User user = userService.findUserById(loginTicket.getUserId());
                // 在本次请求中持有用户（存入ThreadLocal，在controller中通过UserThreadLocal.get直接获取即可）
                UserThreadLocal.put(user);

                // 构建用户认证的结果，并存入 SecurityContext，以便于 Security 进行授权
                Authentication authentication = new UsernamePasswordAuthenticationToken(
                        user, user.getPassword(), userService.getAuthorities(user.getId()));
                SecurityContextHolder.setContext(new SecurityContextImpl(authentication));
            }
        }
        return true;
    }

    @Override
    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {
        // 从 ThreadLocal 中获取该用户，返回给视图层
        User user = UserThreadLocal.get();
        if (user != null && modelAndView != null) {
            modelAndView.addObject("loginUser", user);
        }
    }

    @Override
    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {
        // 在 DispatcherServlet 完全处理完请求后被调用
        // 视图层都用完了，清理掉 ThreadLocal，否则可能有内存泄露的风险（GC 自动清理没有清理掉）
        UserThreadLocal.clear();

//        SecurityContextHolder.clearContext();
    }
}
```





### 2.4 修改个人信息（重点）

修改个人信息不是主要的，重点是需要登录的用户才能进行修改页面的访问。

可以使用上面的拦截器来实现，但是如果以后又有一些页面要登录后才能访问，那加一个需求就去修改拦截器源码吗？如果我又不想拦截了呢，又去把对应的源码删掉？这样也太麻烦了。

所以这里提供另一种拦截方式，通过 **自定义注解** 进行拦截。被这个注解标注的请求方法，就会被这个拦截器拦截。

自定义一个 LoginRequired 注解，里面什么也不用写，只是其一个标注的作用：

```java
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
public @interface LoginRequired {
}
```

然后编写拦截器，拦截带有此注解的方法：

```java
/**
 * @desc: 需要登录才能请求的拦截器
 * 使用注解实现：
 *      当方法中注解了 @LoginRequired 时，说明该方法需要登录后才能请求
 *      本拦截器就是根据注解来拦截对应的方法的
 * @author: AruNi_Lu
 * @date: 2022/10/12
 */
@Component
public class LoginRequiredInterceptor implements HandlerInterceptor {

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
        // 当前拦截的对象是一个方法时，才拦截
        if (handler instanceof HandlerMethod) {
            // 获取当前拦截的方法
            Method method = ((HandlerMethod) handler).getMethod();
            LoginRequired loginRequired = method.getAnnotation(LoginRequired.class);
            // 若当前拦截的方法注解了 @LoginRequired，说明当前方法需要登录，
            // 但 ThreadLocal 取不到用户，说明未登录，需要拦截。
            if (loginRequired != null && UserThreadLocal.get() == null) {
                // 重定向到登录界面，不放行
                response.sendRedirect(request.getContextPath() + "/login");
                return false;
            }
        }
        return true;
    }
}
```

最后也要在 WebMvcConfig 中添加此拦截器。

现在，只需要在需要进行登录拦截的请求方法中加上此注解，就会被该拦截器拦截了。



### 2.5 多级评论功能（重点）

在查看帖子详情页面的时候，需要将所有评论显示出来，当然了，出于性能考虑，评论也是需要进行分页查询的。

在实现多级评论功能之前，还是回忆一下评论表是怎样的：

![image-20230224164904684](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202302241649050.png)

> 添加评论

首先讲添加评论，分为两种，对帖子的评论和对评论的评论。

对帖子的评论：

首先，根据前端页面已知的字段有 entity_type（帖子为 1） 和 entity_id（当前帖子的 id），当用户点击评论的时候，前端就会传一个 Comment 对象过来（自动把已知的字段值赋进 Comment），后端的接口接收一下这个 Comment 对象即可。

![image-20230224171040531](C:\Users\AruNi\AppData\Roaming\Typora\typora-user-images\image-20230224171040531.png)

后端接收到 Commemt 后，只需要将没有赋值的字段赋上即可，例如有触发评论的 user_id（通过 ThreadLocal 获取）、评论时间等。

最后，评论需要触发评论事件（通过被评论的用户）和添加帖子的 score（更新热榜），所以需要把评论事件丢进 Kafka、把该帖子丢进 Redis，再由定时任务从 Redis 中取出，更新 ES。







### 2.6 过滤敏感词

过滤敏感词主要是为了发帖时和评论时对敏感词进行过滤，过滤敏感词使用前缀树比较方便，可以依次比较某个区间的内容是否为敏感内容。

> 什么是前缀树？

Trie 树也称为前缀树、字典树，最大的特点就是 **共享字符串的公共前缀** 来达到节省空间的目的。

Trie 树的根节点不存储数据，每一条完整的分支代表一个完整的字符串。

**Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起**。

举例：

有 6 个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在。如果每次查找，都是拿要查找的字符串跟这 6 个字符串依次进行字符串匹配，那效率就比较低，有没有更高效的方法呢？

这个时候，我们就可以先对这 6 个字符串做一下预处理，组织成 Trie 树的结构，之后每次查找，都是在 Trie 树中进行匹配查找。最后构造出来的就是下面这个图中的样子：

![image-20221029131606877](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210291316198.png)

所以利用前缀树的特点，可以很方便的进行敏感词过滤的设计。



> 敏感词过滤设计

在设计敏感词过滤的过程中，我们可以这样定义Trie树：

![image-20221029131855347](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210291318306.png)

图中的红叉表示标记到这个字符，从 `root `到此字符所经过的字符串是敏感词。

把所有敏感词存入Trie树中，设置三个指针：p1、p2、p3。

其中：p1 指向 Trie 树的根节点，p2 和 p3 指向待过滤的字符数组。

过滤过程算法如下：

1. p1 找到 root 下一层，看有没有与 p2 指向相同的字符，若没有则说明以 p2 位置开始的字符不是敏感词，p2 和 p3 都向后移动；若有则说明 p2 位置开始的字符可能是敏感词，p2 固定不动，p3 向后移动，然后重复上述过程；
2. 如果 p1 指向了带标记的节点，那么说明 p2 ~ p3 区间的字符是敏感词，将 p2 ~ p3 区间的字符串用 *** 代替并存入结果数组中，此时将 p2 和 p3 都移动到 p3 + 1 的位置，开始下一轮的过滤；
3. 如果 p1 没有指向带标记的节点，说明 p2 ~ p3 区间的字符不是敏感词，则将 p2++，p3 = p2，重新开始下一轮的过滤；
4. 当 p3 == 数组长度时，将剩余字符加入结果数组；



复杂度分析：

- 如果敏感词的长度为 m，则每个敏感词的查找时间复杂度是 O(m)，字符串的长度为 n，我们需要遍历 n 遍，所以敏感词 **查找** 这个过程的时间复杂度是 **O(n * m)**。
- 如果有 t 个敏感词的话，**构建** trie 树的时间复杂度是 **O(t * m)**。



> 代码实现

从上图可以看到，每个节点都保存了当前节点的字符值和它的子节点，所以可以使用 Map 来表示每个节点，key 为字符，value 为子节点；因为一个节点的子节点个数未知，采用 HashMap 还可以动态拓展，而且可以在 O(1) 复杂度内判断某个子节点是否存在。

所以，前缀树的结构定义如下：

```java
    // 前缀树结构的定义
    private class TrieNode {

        // 关键词结束标识（以关键词结尾的字符的此属性为 true）
        private boolean isKeyWordEnd  = false;

        // 子节点集合（key：下级字符，value：下级节点）
        private Map<Character, TrieNode> subNodesMap = new HashMap<>();

        public void setKeyWordEnd(boolean keyWordEnd) {
            isKeyWordEnd = keyWordEnd;
        }

        // 添加子节点
        public void addSubNode(Character c, TrieNode node) {
            subNodesMap.put(c, node);
        }

        // 获取子节点
        public TrieNode getSubNode(Character c) {
            return subNodesMap.get(c);
        }
    }
```

我们需要提前初始化好前缀树，以便进行敏感词的匹配过滤，所以需要先写一个添加敏感词到前缀树的方法：

```java
    // 把一个敏感词添加到前缀树中
    private void addKeyword(String keyword) {
        // 定义一个变量指向根节点，从根开始添加
        TrieNode curNode = rootNode;
        for (int i = 0; i < keyword.length(); i++) {
            char c = keyword.charAt(i);
            TrieNode subNode = curNode.getSubNode(c);
            // 如果当前字符还没有子节点，说明前缀树中没有该字符，需要初始化，
            // 否则沿着子节点继续添加下一个字符
            if (subNode == null) {
                // 初始化子节点，接到当前节点上
                subNode = new TrieNode();
                curNode.addSubNode(c, subNode);
            }

            // 指向子节点，进入下一轮循环
            curNode = subNode;
        }
        // 设置结束标识
        curNode.setKeyWordEnd(true);
    }
```

在项目启动的时候，需要根据敏感词文件中的敏感词来初始化前缀树，利用 Spring 的 @PostConstruct 注解，在 Bean 被实例化，调用构造器之后，就初始化前缀树：

```java
    // 在 bean 被实例化，调用构造器之后，@PostConstruct 标注的方法就会被自动调用
    @PostConstruct
    // 初始化前缀树
    public void init() {
        try (
                // 从类加载器中获取加载好的资源（敏感词文件），然后将字符流转为缓冲流读取该文件
                InputStream is = this.getClass().getClassLoader().getResourceAsStream("sensitive-words.txt");
                BufferedReader reader = new BufferedReader(new InputStreamReader(is));
        ) {
            String keyword;
            // 一行一行的读取
            while ((keyword = reader.readLine()) != null) {
                // 添加到前缀树
                this.addKeyword(keyword);
            }
        } catch (Exception e) {

        }
    }
```

接下来就只剩下过滤的核心方法了，这个方法需要在业务中使用，具体实现如下：

```java
    /**
     * 过滤敏感词（供外界调用）
     * @param text 待过滤的文本
     * @return 过滤后的文本
     */
    public String filter(String text) {
        if (StringUtils.isBlank(text)) {
            return null;
        }
        StringBuilder sb = new StringBuilder();
        // 指针1：用于遍历前缀树
        TrieNode curNode = rootNode;
        // 指针2：遍历文本的左边界
        int begin = 0;
        // 指针3：遍历文本的右边界（指针2，3 形成的区间的文本可能是敏感词）
        int pos = 0;

        // 过滤算法：遍历文本的过程中，同时遍历整颗前缀树，找出符合的文本
        while (pos < text.length()) {
            char c = text.charAt(pos);
            // 跳过符号（“聪明”的网友：跟我来%赌$博%）
            if (isSymbol(c)) {
                // 若指针1 处于根节点，将此符号计入结果，让指针2 向下走
                if (curNode == rootNode) {
                    sb.append(c);
                    begin++;
                }
                // 无论符号在开头还是中间，指针3 都向下走
                pos++;
                continue;
            }

            // 检查下级节点
            curNode = curNode.getSubNode(c);
            // 下级节点为空，表示以 begin 开头的字符不是敏感词
            if (curNode == null) {
                sb.append(text.charAt(begin));
                // 进入下一个位置
                pos = ++begin;
                // 重新指向根节点，开始下一个字符的判断
                curNode = rootNode;
            } else if (curNode.isKeyWordEnd) {
                // 发现敏感词，替换
                sb.append(REPLACEMENT);
                // 进入下一个位置
                begin = ++pos;
                // 重新指向根节点
                curNode = rootNode;
            } else {
                // 检查下一个字符
                pos++;
            }
        }
        // 由于循环是以指针3 pos（右区间）为结束条件，所以最后可能还有区间没被加入进结果中
        sb.append(text.substring(begin));
        return sb.toString();
    }

    // 判断是否为符号
    private boolean isSymbol(Character c) {
        // isAsciiAlphanumeric() 判断是否为数字或希腊字母 | 0x2E80-0x9FFF 之间的字符是东亚文字
        return !CharUtils.isAsciiAlphanumeric(c) && (c < 0x2E80 || c > 0x9FFF);
    }
```



### 2.7 统一异常处理

SpringBoot 对于异常的处理也做了不错的支持，它提供了 2 个注解：

- **@ControllerAdvice**：用来 **开启全局的异常捕获**，说明要在哪些地方捕获；
- **@ExceptionHandler**：说明 **捕获哪些异常**，对那些异常进行处理；

统一异常处理利用了 AOP 的思想，在没有改变原有代码的情况下实现了异常处理。

```java
/**
 * @desc: @ControllerAdvice作用：给 Controller 控制器添加统一的操作或处理。
 * 对加了 @Controller 注解的控制器添加统一的异常处理（AOP实现）
 * @author: AruNi_Lu
 * @date: 2022/10/15
 */
@ControllerAdvice(annotations = Controller.class)
public class AllExceptionHandler {

    private static final Logger logger = LoggerFactory.getLogger(AllExceptionHandler.class);

    // 进行异常处理，处理 Exception.class 的异常(全部异常)
    @ExceptionHandler({Exception.class})
    public void handlerException(Exception e, HttpServletRequest request, HttpServletResponse response) throws IOException {
        logger.error("服务器发生异常：" + e.getMessage());
        for (StackTraceElement element : e.getStackTrace()) {
            logger.error(element.toString());
        }

        String xRequestWith = request.getHeader("x-requested-with");
        // 判断是异步还是普通请求，异步请求需要返回 JSON 字符串，普通请求返回错误页面
        if ("XMLHttpRequest".equals(xRequestWith)) {
            response.setContentType("application/plain;charset=utf-8");
            PrintWriter writer = response.getWriter();
            writer.write(JSONUtil.getJSONString(1, "服务器异常！"));
        } else {
            response.sendRedirect(request.getContextPath() + "/error");
        }
    }
}
```



### 2.8 统一日志管理

Spring 对统一日志管理做了集合，使用的是 logback，只需要在 resource 下创建 `logback-spring.xml` 配置文件配置日志的记录格式，日志类型等等，就可以直接在项目中使用。

而我们现在自己定义一个日志，来记录每个用户在什么时间点访问了业务层的什么方法。

Spring 提供了比较方便的方式，不用动原本的业务层代码，也是使用 AOP 思想，将切点定义为需要记录日志的地方即可实现。

首先来了解一下 AOP 的几个核心概念：

- 通知（Advice）：就是 **自己想要的功能**，比如我们这里要打印日志。
- 连接点（JoinPoint）：就是 **使用通知的地方**，Spring 只支持方法连接点，方法执行前/后、return 之后、抛出异常之后都是连接点。
- 切点（Pointcut）：有时候我们不希望在所有的方法上都使用 Advice，而 Pointcut 就是提供一组规则来匹配 JoinPoint，给满足规则的 JoinPoint 使用 Advice。
- 切面（Aspect）：通知和切点的结合。其实连接点就是为了方便理解切点而定义出来的。通知说明了干什么和什么时候干（什么时候干在定义通知的时候可以指定），而切入点说明了在哪干（指定到底是哪个方法），这就是一个完整的切面定义。
- 织入（Weaving）：把切面应用到目标对象来创建新的代理对象的过程，Spring 中用的是运行时。



通知有以下几种类型：

- @Before("pointcur()")：在执行切点之前需要做的通知；
- @After("pointcur()")：在执行切点之后需要做的通知；
- @AfterReturning("pointcur()")：在返回值之后需要做的通知；
- @AfterThrowing("pointcur()")：在抛出异常后需要做的通知；
- @Around("pointcur()")：环绕通知，可在切点前后做通知，

定义方法时参数可以传入 JoinPoint，用来执行被通知的方法（`joinPoint.process()`）或者获取被执行的方法（`joinPoint.getSignature().getName()`）。



根据我们的业务需求，在 service 层方法执行前，记录下访问记录即可，所以切点定义的规则定义为 `service.Impl` 包下的所有方法都要使用通知。具体的通知内容在执行切点之前做，所以使用 @Before。

```java
/**
 * @desc: 利用 AOP，记录用户访问业务方法的日志
 * @author: AruNi_Lu
 * @date: 2022/10/15
 */
@Component
@Aspect
public class ServiceLogAspect {

    private static final Logger logger = LoggerFactory.getLogger(ServiceLogAspect.class);

    // 切点，让 service.impl 包下的所有方法执行通知，execution 格式：返回值 包下的方法(参数)
    @Pointcut("execution(* com.run.nowcoder.service.impl.*.*(..))")
    public void pointcut() {
    }

    // 在执行切点之前需要做的通知
    @Before("pointcut()")
    public void before(JoinPoint joinPoint) {
        // 用户[ip]，在[now_time]，访问了[com.run.nowcoder.service.impl.xxx()]
        // 利用 RequestContextHolder 工具的 getRequestAttributes() 方法获取请求上下文
        ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();
        // 根据请求上下文获取 request
        if (attributes == null) return;
        HttpServletRequest request = attributes.getRequest();
        // 根据 request 获取 ip 地址
        String ip = request.getRemoteHost();
        String now = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date());
        // 获取方法类型名 + 方法名
        String target = joinPoint.getSignature().getDeclaringTypeName() + "." + joinPoint.getSignature().getName();
        logger.info(String.format("用户[%s]，在[%s]，访问了[%s]", ip, now, target));
    }
}
```



## 3. Redis 相关

### 3.1 点赞（重点）

点赞业务本身并不复杂，无非是对数据的 update，但是点赞本身是无意识行为，并且同一个用户可对博文进行点赞/取消点赞，如果直接操作数据库，无疑会增加数据库 io 操作。

所以这里使用 Redis 来存储点赞数据。

对于数据类型的选用：

- 由于我们需要快速判断该用户是否对帖子/评论点过赞，所以需要判断某个元素是否在集合中，因此使用 Set，它提供了一个 `SISMEMBER` 命令来判断元素是否在集合中；

对于 key 的设计：

- 我们现在有对帖子的点赞和对评论的点赞，或许后面还会开放对其他实体的点赞。所以我们要设置一个实体类型 entityType，来区分点赞的类型；
- 然后在实体类型后面跟上该实体的 id 即可（postId / commentId）；
- 所以点赞功能的 key 为 like:entity:entityType:entityId；
- value 为给该实体点赞的用户 id



由于我们还要统计每个用户获得的总赞数，所以在记录点赞时也要顺便把用户的总赞数 +1。

每个用户的总赞数用 String 数据类型来储存，key 为 like:user:userId，value 为获得的赞总数。



因为点赞和统计用户赞的总数是一组不可分割的行为，所以需要保证原子性，因此使用事务来执行这两条命令。



至于这个点赞操作的实体类型，以及实体 id 和 被点赞人的 id，都是从前端传过来，而对于点赞者可以通过 ThreadLocal 来获取。



### 3.2 关注（重点）

关注和点赞非常类似，关注操作需要更新关注者的关注信息，还要更新被关注者的关注信息，所以也需要在事务中操作。

对于关注操作，我们使用 zSet 来存储：

- 因为在显示关注列表和粉丝列表时，一般都是按照时间降序显示的；
- 而且方便后续添加共同关注的功能，zSet 也支持取交集；



某个用户关注的实体，key 的设计：

- 除了关注用户外，以后还有可能关注某个频道、关注某个标签等等功能，所以我们这里不能把 key 设计死，也需要根据类型来设计；
- 所以 key 为某个用户所关注的实体：`followee:useId:entityType`；value 为这个实体所对应的 id，和关注的时间：`zSet(entityId, nowTime)`；



某个实体所拥有的粉丝，key 的设计：

- 同上面类似，key 为某个实体（实体类型和实体 id 唯一确定一个实体），value 为该实体的粉丝集合（粉丝其实就是用户）；

- 所以，key：`follower:entityType:entityId`；value：`zSet(userId, nowTime)`；



### 3.3 页面优化

我们有一些访问数据库的操作是非常频繁的，例如在进入首页的时候就要查询一整页的帖子信息，帖子确实可以一次性查出一页的数据来，但是每个帖子对应的作者则需要查询多次，有多少个帖子就要查询多少次，这样频繁的访问数据库效率是比较低的，所以我们考虑用 Redis 来做缓存。

查询用户时，根据下面三步走：

1. 当查询时，优先从缓存中取值；
2. 取不到时再走数据库，将查出的数据存入缓存中再返回；
3. 数据更新时要及时清除缓存。注意，这里不是采用更新缓存，而是直接清除。因为若更新数据库频繁，而缓存的数据不一定会频繁使用，所以删除比更新开销小。



缓存代码如下：

```java
    /*
    优化：使用 Redis 缓存登录的用户，这样在页面间访问时，可以直接从缓存中取，而不用每次都访问数据库，
    缓存只保留一小时，用户下次请求时若缓存没有命中，则冲数据库中查出数据，放入缓存再返回。
     */
    // 1.当查询时，优先从缓存中取值
    private User getCache(int userId) {
        String userKey = RedisKeyUtil.getUserKey(userId);
        return (User) redisTemplate.opsForValue().get(userKey);
    }

    // 2.取不到时初始化缓存数据
    private User initCache(int userId) {
        // 先从数据库中查询，存入缓存后再返回
        User user = userMapper.selectById(userId);
        String userKey = RedisKeyUtil.getUserKey(userId);
        redisTemplate.opsForValue().set(userKey, user, 3600, TimeUnit.SECONDS);
        return user;
    }

    // 3.数据更变时清除缓存（不更新缓存是因为若更新数据库频繁，而缓存的数据不一定会频繁使用，所以删除比更新开销小）
    public void clearCache(int userId) {
        String userKey = RedisKeyUtil.getUserKey(userId);
        redisTemplate.delete(userKey);
    }
```

在需要查询用户的地方优先使用缓存即可：

```java
    @Override
    public User findUserById(int id) {
//        return userMapper.selectById(id);

        // 优化：先从缓存中查询用户，查不到则初始化缓存
        User user = getCache(id);
        return user != null ? user : initCache(id);
    }
```

在更新用户信息的地方记得都要调用清除缓存的方法。



### 3.4 Redis 高级数据类型

下面介绍两种 Redis 高级数据类型，用于统计网站的 UV(Unique Visitor) 和 DAU(Daily Active User)。



> HyperLogLog

HyperLogLog 是用来做基数统计的，所谓基数统计，就是指一串数字中不重复的数字个数，如 {1，2，1，2，3} 的基数集就是 {1，2，3}，基数就是3。



基本操作：

```bash
# 添加数据
pfadd key element [element ...] 

# 统计数据
pfcount key [key ...]

# 合并数据
pfmerge destkey sourcekey [sourcekey...]
```



HyperLogLog 特点：

- 用于进行基数统计，不是集合，不保存数据，只记录数量而不是具体数据
- 核心是基数估算算法，最终数值存在一定误差

- 误差范围：基数估计的结果是一个带有 0.81% 标准错误的近似值
- 耗空间极小，每个 hyperloglog key 占用了 12K 的内存用于标记基数
- pfadd 命令不是一次性分配 12K 内存使用，会随着基数的增加内存逐渐增大

- Pfmerge命令合并后占用的存储空间为 12K，无论合并之前数据量多少



业务场景：

- 统计页面实时 UV 数、统计在线用户数、统计用户每天搜索不同词条的个数。
- 而 BitMaps（下面说）则用于判断某个用户是否访问过页面。



> BitMaps

介绍：

- BitMaps 就是通过一个 bit 位来表示某个元素对应的值或者状态，其中的 key 就是对应元素本身；
- 可以把 Bitmaps 想象成是一串二进制数字，每个位置只存储 0 或 1（某种状态），下标是 Bitmaps 的偏移量（offset）；
- 因为一个字节有 8 位，所以 BitMaps 本身会极大的节省存储空间；



基本操作：

```bash
# 获取指定 key 对应偏移量上的 bit 值
getbit key offset       # 如果没有设置的话，默认是 0

# 设置指定 key 对应偏移量上的 bit 值，value 只能是 0 或 1
setbit key offset value      # 偏移量很大的话，也能设置，但是前面要补 0，比较耗时
 
# 对指定 key 按位进行交、并、非、异或操作，并将结果保存到 destKey 中
bitop op destKey key1 [key2...]         # op：and（交）、or（并）、not（非）、xor（异或）
  
 # 统计指定 key 中 1 的数量
  bitcount key [start end]
```



业务场景：

- 统计用户在指定范围内登录过多少次；
- 统计指定日期范围内的 DAU 数量；
- 统计每天网站的 PV（Page Visitor），不能统计 UV，因为 UV 要去重；



> UV 和 DAU 的实现

使用 Redis，首先都要设计好 key，由于我们的需求是既可以对单日进行统计，也可以对某个区间范围进行统计，所以需要设计两种类型的 key。

对于 UV，采用 HyperLogLog：

- 单日 UV 的 key 为 uv:date，value 为当天访问过网站的 ip 地址集合；
- 区间 UV 的 key 为 uv:startDate:endDate，value 为 ip 地址集合；（区间 UV 其实就是把范围日期内的单日 UV 进行合并，再存入区间 UV 中）

对于 DAU，采用 BitMap：

- 单日 DAU的 key 为 dau:date，offset 为 userId，value 为 0|1；
- 区间 DAU的 key 为 dau:startDate:endDate，offset 为在指定日期范围内，活跃用户的 userId，value 为 0|1；（区间 DAU 其实就是把范围日期内的单日 DAU 进行  OR 运算，再存入区间 DAU 中）

注：单日 UV 和 DAU 只是用于在用户访问的时候记录进 Redis，我们的业务主要是范围统计。而范围统计要靠每一天的 UV 和 DAU 合并计算出来的。



key 设计好了，那么我们什么时候进行统计呢？在哪儿统计呢？

肯定要在请求时进行统计，那么可以用拦截器实现，在用户访问页面时，必然先走拦截器：

```java
@Component
public class DataInterceptor implements HandlerInterceptor {
    @Autowired
    private DataService dataService;

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
        // 统计 UV
        String ip = request.getRemoteHost();
        dataService.recordUV(ip);

        // 统计 DAU
        User user = UserThreadLocal.get();
        if (user != null) {
            dataService.recordDAU(user.getId());
        }
        return true;
    }
}
```

主要的统计业务，范围统计：

```java
    /**
     * 统计指定日期范围内的 UV 数量
     * @param start 开始时间
     * @param end 结束时间
     * @return
     */
    @Override
    public long calculateUV(Date start, Date end) {
        if (start == null || end == null) {
            throw new IllegalArgumentException("参数不能为空！");
        }

        // 整理该日期范围内的 key
        List<String> keyList = new ArrayList<>();
        // Calendar 工具可用于处理跟时间相关的逻辑
        Calendar calendar = Calendar.getInstance();
        calendar.setTime(start);
        // 当得到的时间不在结束时间之后时，统计每一天的key，且将时间 +1天，继续下一次循环统计
        // 不用 .before(end) 是因为会漏掉 end 时间
        while (!calendar.getTime().after(end)) {
            String key = RedisKeyUtil.getUVKey(sdf.format(calendar.getTime()));
            keyList.add(key);
            calendar.add(Calendar.DATE, 1);
        }

        // 合并这些数据，存入 范围UV 的 key 中
        String redisKey = RedisKeyUtil.getUVKey(sdf.format(start), sdf.format(end));
        redisTemplate.opsForHyperLogLog().union(redisKey, keyList.toArray());

        // 返回统计结果
        return redisTemplate.opsForHyperLogLog().size(redisKey);
    }


    /**
     * 统计指定日期范围内的 DAU 数量
     * @param start 开始时间
     * @param end 结束时间
     * @return
     */
    @Override
    public long calculateDAU(Date start, Date end) {
        if (start == null || end == null) {
            throw new IllegalArgumentException("参数不能为空！");
        }

        // 整理该日期范围内的 key，BitMap存储的key类型是字节数组
        List<byte[]> keyList = new ArrayList<>();
        // Calendar 工具可用于处理跟时间相关的逻辑
        Calendar calendar = Calendar.getInstance();
        calendar.setTime(start);
        // 当得到的时间不在结束时间之后时，统计每一天的key，且将时间 +1天，继续下一次循环统计
        // 不用 .before(end) 是因为会漏掉 end 时间
        while (!calendar.getTime().after(end)) {
            String key = RedisKeyUtil.getDAUKey(sdf.format(calendar.getTime()));
            keyList.add(key.getBytes());
            calendar.add(Calendar.DATE, 1);
        }

        // 进行 OR 运算，日期范围内任意一天登录过都算活跃，存入范围DAU 的 key 中
        Object dauCnt = redisTemplate.execute((RedisCallback) connection -> {
            String redisKey = RedisKeyUtil.getDAUKey(sdf.format(start), sdf.format(end));
            connection.bitOp(RedisStringCommands.BitOperation.OR,
                    redisKey.getBytes(), keyList.toArray(new byte[0][0]));
            // 返回 key 为 范围DAU 中位为1的数量
            return connection.bitCount(redisKey.getBytes());
        });
        return (long) dauCnt;
    }
```

Controller 层接收前端传入的时间，查询到统计结果后响应即可：

```java
@Controller
public class DataController {

    @Autowired
    private DataService dataService;

    // 跳转到统计页面，支持 POST 请求是为了让下面的 /data/uv 能转发过来
    @RequestMapping(path = "/data", method = {RequestMethod.GET, RequestMethod.POST})
    public String getDataPage() {
        return "/site/admin/data";
    }

    // 统计网站 UV，@DateTimeFormat 将前端传来的字符串类型的日期转为我们指定的格式
    @RequestMapping(path = "/data/uv", method = RequestMethod.POST)
    public String getUV(@DateTimeFormat(pattern = "yyyy-MM-dd") Date start,
                        @DateTimeFormat(pattern = "yyyy-MM-dd") Date end, Model model) {
        long uv = dataService.calculateUV(start, end);
        model.addAttribute("uvResult", uv);
        model.addAttribute("uvStartDate", start);
        model.addAttribute("uvEndDate", end);
        /* 此处可以用转发：转发到 /data 页面，此页面的信息会被带到转发的页面，转发是在一个请求中完成的，
        要求转发的请求与原请求类型相同，这也是为什么上面写 /data 请求时要支持 POST；*/
        // 不能用重定向：因为重定向相当于客户端重新单独发了一次 /data 请求，此页面的信息不会携带过去；
        return "forward:/data";
    }

    // 统计网站 DAU
    @RequestMapping(path = "/data/dau", method = RequestMethod.POST)
    public String getDAU(@DateTimeFormat(pattern = "yyyy-MM-dd") Date start,
                        @DateTimeFormat(pattern = "yyyy-MM-dd") Date end, Model model) {
        long uv = dataService.calculateDAU(start, end);
        model.addAttribute("dauResult", uv);
        model.addAttribute("dauStartDate", start);
        model.addAttribute("dauEndDate", end);
        return "forward:/data";
    }
}
```



## 4. Kafka 相关

### 4.1 什么是 Kafka

Kafka 的核心概念：

- 生产者（Producer）与消费者（Consumer）：
  - 生产者（也称为发布者）创建消息；
  - 而消费者（也称为订阅者）负责消费 or 读取消息；
- 主题（Topic）与分区（Partition）：
  - 在 Kafka 中消息以主题来分类，每一个主题都对应一个「消息队列」。
  - 一个 Topic 被分成多个 Partition，Partition 是最小的 **存储单元**，掌握着一个 Topic 的部分数据。
- Broker 和集群（Cluster）：
  - 一个 Kafka 服务器也称为 Broker，它接受生产者发送的消息并存入磁盘。
  - Broker 同时服务消费者拉取分区消息的请求，返回目前已经提交的消息。
  - 一个 Broker 每秒可以处理成千上万的分区和百万量级的消息。
  - 若干个 Broker 组成一个集群（Cluster），其中集群内某个 Broker 会成为集群控制器（Cluster Controller），它负责管理集群。

![img](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210301628001.webp)

Kafka特点：

- 高吞吐量、消息持久化、高可靠性、高扩展性。



### 4.2 发送系统通知

其实系统通知和发普通私信是一样的，只是消息的发送者是系统而已。

那么为什么系统通知要使用 Kafka 呢？

- 系统通知是触发点赞，关注，评论等功能时，通知被动方。可能在某一时间点一个热贴的点赞或评论很多，这时如果每次都讲通知保存在数据库，那并发量太大了，系统性能明显会下降。
- 所以利用消息队列，当触发点赞，关注，评论等功能时，就把相应的通知丢到消息队列中，让系统异步的去消费消息（发通知）。



> 规定 Topic 和 content

很明显，每类通知都要对应一种 Topic：

- 评论 Topic：comment
- 点赞 Topic：like
- 关注 Topic：follow



Topic 对应的 content 定义为一个 JSON 字符串，我们将其封装成一个 Event 实体对象便于 JSON 的转化：

```java
public class Event {
    private String topic;
    private int userId;
    private int entityType;
    private int entityId;
    private int entityUserId;
    // 其他数据信息
    private Map<String, Object> data = new HashMap<>();
}
```



因为系统通知也是消息的一种，所以我们将系统通知也存入 message 表中，message 表如下：

![image-20221028144604816](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210281446446.png)

在系统消息中，各字段的含义如下：

- from_id：固定为 1，表示系统管理员；
- to_id：要通知的用户 id；
- conversation_id：由于系统通知分为三大类，所以有三种不同的值，**每种值对应 Kafka 的 Topic**：
  - follow：关注通知
  - like：点赞通知
  - comment：评论通知
- content：JSON 字符串，具体内容根据通知的类型而定：
  - follow 类型：{"entityType": 通知类型，3 表示关注，"entityId"：被通知的实体 id(对于关注来说是用户 id)，"userId": 触发通知的用户 id}
  - like 类型：{"entityType": 通知类型，2 表示点赞，"entityId": 被通知的实体 id(对于点赞来说是帖子 id)，"postId": 被点赞的帖子 id，"userId": 触发点赞的用户 id}
  - comment 类型：{"entityType": 通知类型，1 表示评论，"entityId"：被评论的实体 id(对于评论来说是帖子 id，或者被评论的评论 id)，"postId": 被评论的帖子 id，"userId": 触发评论的用户 id}



> Kafka 的使用

想要使用 Kafka，首先得先定义一个事件的生产者，用在我们需要发送通知的地方，把事件丢给生产者就行了。然后再为 Topic 定义一个消费者，监听生产者的队列，有事件就进行消费即可。

生产者的定义如下：

```java
@Component
public class EventProducer {
    @Autowired
    private KafkaTemplate kafkaTemplate;

    // 处理事件（发消息）
    public void fireEvent(Event event) {
        // 将事件发布到指定的主题，内容是事件对象的 JSON 字符串
        kafkaTemplate.send(event.getTopic(), JSONObject.toJSONString(event));
    }
}
```

接着，在需要发送通知的地方封装事件，丢给生产者。例如在添加评论时，把评论事件丢给生产者：

```java
    @RequestMapping(path = "/add/{discussPostId}", method = RequestMethod.POST)
    public String addComment(@PathVariable("discussPostId") int discussPostId, Comment comment) {
        comment.setUserId(UserThreadLocal.get().getId());
        comment.setStatus(0);
        comment.setCreateTime(new Date());
        commentService.addComment(comment);

        // 触发评论事件（通知给被评论的用户），EntityType，EntityId 等信息前端已经赋上了
        Event event = new Event()
                .setTopic(CommonConstant.TOPIC_COMMENT)
                .setUserId(UserThreadLocal.get().getId())
                .setEntityType(comment.getEntityType())
                .setEntityId(comment.getEntityId())
                .setData("postId", discussPostId);
        // 由于不知道评论的目标是子评论还是帖子，所以需要判断
        if (comment.getEntityType() == CommonConstant.ENTITY_TYPE_POST) {
            DiscussPost target = discussPostService.findDiscussPostById(comment.getEntityId());
            event.setEntityUserId(target.getUserId());
        } else if (comment.getEntityType() == CommonConstant.ENTITY_TYPE_COMMENT) {
            Comment target = commentService.findCommentById(comment.getEntityId());
            event.setEntityUserId(target.getUserId());
        }
        eventProducer.fireEvent(event);
        
        return "redirect:/discuss/detail/" + discussPostId;
    }
```

现在消息事件已经被生产者接收，放入队列中了，接下来就要定义消费者去消费这个 Topic 中的事件了。由于评论、点赞、关注都是属于系统通知，所以可以用一个消费者消费这三类 Topic：

```java
    /**
     * 消费 评论/点赞/关注 事件
     * @param record
     */
    @KafkaListener(topics = {CommonConstant.TOPIC_COMMENT, CommonConstant.TOPIC_LIKE, CommonConstant.TOPIC_FOLLOW})
    public void handlePDGMessage(ConsumerRecord record) {
        if (record == null || record.value() == null) {
            logger.error("消息的内容为空");
            return;
        }

        // 从队列中获取消息
        Event event = JSONObject.parseObject(record.value().toString(), Event.class);
        if (event == null) {
            logger.error("消息格式错误");
            return;
        }

        // 发送站内通知
        Message message = new Message();
        message.setFromId(1);   // 系统通知的id为1
        message.setToId(event.getEntityId());       // 通知目标实体的id
        message.setConversationId(event.getTopic());
        message.setCreateTime(new Date());

        // message 中的内容（content），JSON 字符串类型
        Map<String, Object> content = new HashMap<>();
        content.put("userId", event.getUserId());
        content.put("entityType", event.getEntityType());
        content.put("entityId", event.getEntityId());

        // 其他数据消息，通过 event.getData()（Map） 来获取
        if (!event.getData().isEmpty()) {
            content.putAll(event.getData());
        }

        message.setContent(JSONObject.toJSONString(content));
        messageService.addMessage(message);
    }
```

从代码中可以看到，消费者消费消息的步骤为：

1. 从对应 Topic 的队列中获取消息事件；
2. 将事件的内容读取出来，插入到 message 表中；
3. 用户在访问页面时，就会访问 MessageController 中的获取通知的请求方法，将系统通知和私信都读出来，最后显示在页面上，就完成了通知功能。

> 面试被问到：
>
> 用户每次都需要刷新才能看到新的通知？有没有什么好的解决方案？
>
> 可以让客户端使用异步请求的形式从服务端获取新的通知，这样在客户端看来就像没有刷新一样。
>
> 那就是要轮询？你知道 WebSocket 吗？
>
> WebSocket 是一种服务端主动向客户端推送数据的技术。



除了系统通知，Kafka 还用于更新 ES（帖子的增删改后都要更新 ES） ，或者一些对实时性要求没有那么高的功能。



## 5. ElasticSearch 相关

### 5.1 为什么要用 ES

我们使用 ES 最多的就是搜索功能了，数据库也能做搜索功能啊，比如：
```sql
SELECT * FROM `discuss_post` where content like '%寒冬%';
```

这样也能把 "寒冬" 相关的内容的帖子搜索出来，但是像 content like '%寒冬%' 这类查询是不走索引的。

从 SQL 执行的分析结果中可以看出，上面的查询语句没有可用的索引，且查询类型为 ALL（全表扫描）：

![image-20221030182241344](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210301822836.png)

你可能会说，那好办，那我在 content 列添加一个索引呗。但是 content 列的类型为 text，我们需要给出前缀索引长度才能为此列建立索引，因为索引列的长度是有限制的。那如果我要查询的关键字就是在 content 中的后面部分呢？你只建立前缀索引有什么用？所以建立 content 索引列不是很方便。

还有一个解决办法，MySQL 5.6 版本开始，InnoDB 也支持全文索引（全文索引有自己的语法格式），5.7.6 之后也有了中文的全文分析器 ngram。所以我们可以在 content 列建立全文索引。

但是 MySQL 的全文索引有些性能问题：

- 当搜索的结果集较大时，会引发执行异常，需要调高相关内存参数配置；
- 输入目标文本越长，性能越低；
- 符合条件记录数越多，性能越低；

而且，我们有时候也不需要将符合条件的结果全部返回给用户，可能用户就查看前面一两页的数据，所以要求前面的数据一定要是相关性最强的。

所以，专业的事（全文搜索）还是交给专业的人（ES）来做吧！



### 5.2 什么是 ES

> ES 能干什么

我们主要讨论它的核心功能 — 搜索：

- Elasticsearch 对模糊搜索非常擅长（搜索速度很快）
- 从Elasticsearch 搜索到的数据可以根据 **评分** 过滤掉大部分的，只要返回评分高的给用户就好了（原生就支持排序）
- 没有那么准确的关键字也能搜出相关的结果（能匹配有相关性的记录）



> ES 的核心概念

节点（Node）、集群（Cluster）、分片（Shards）：

- 节点（Node）：单个实例称为一个节点；
- 集群（Cluster）：一组节点构成一个集群；
- 分片（Shards）：分片是底层的工作单元，文档保存在分片内，分片又被分配到集群内的各个节点里，每个分片仅保存全部数据的一部分。



索引 Index、类型 Type 和文档 Document，对比我们比较熟悉的 MySQL 数据库：

- index → db
- ~~type → table~~
- document → row

**注意**：在 ES 7.0 之后的版本中 Type 被废弃了。一个 index 中只有一个默认的 type，即 _doc。

ES 的 Type 被废弃后，库表合一，Index 既可以被认为对应 MySQL 的 Database，也可以认为对应 table。

所以 ES 7.0 之后可以这样认为：

- ES 实例：对应 MySQL 实例中的一个 Database。
- Index 对应 MySQL 中的 Table 。
- Document 对应 MySQL 中表的记录



### 5.3 搜索功能

接下来，我们就利用 ES 来实现搜索功能。

> ES 客户端配置

首先要建立一个 ES 客户端，用于连接 ES，然后在这个客户端上进行复杂的查询操作：

```java
@Configuration
public class EsConfig {
    @Value("${spring.elasticsearch.uris}")
    private String esUrl;

    @Bean
    RestHighLevelClient esClient() {
        ClientConfiguration clientConfiguration = ClientConfiguration.builder()
                .connectedTo(esUrl)
                .build();
        return RestClients.create(clientConfiguration).rest();
    }
}
```

在需要使用的地方用 `@Qualifier("esClient")` 指定类名注入即可。



还要将数据存入到 ES 中，可以使用  ElasticsearchRepository 接口，只要继承它就可以使用了：

```java
@Repository
public interface DiscussPostRepository extends ElasticsearchRepository<DiscussPost, Integer> {
}
```



除此之外，为了方便，我们创建一个 SearchResult 实体类，用于存放查询结果：

```java
/**
 * @desc: 用于暂存 es 中查询到的列表和总行数
 * @author: AruNi_Lu
 * @date: 2022/10/20
 */
public class SearchResult {
    private List<DiscussPost> list;

    private long total;

    public SearchResult(List<DiscussPost> list, long total) {
        this.list = list;
        this.total = total;
    }
}
```



> 功能实现

在 service 层提供数据存入 ES，从 ES 删除数据，查询数据的方法：

```java
@Service
public class ElasticsearchServiceImpl implements ElasticsearchService {

    @Autowired
    private DiscussPostRepository postRepository;

    @Autowired
    @Qualifier("esClient")
    private RestHighLevelClient restHighLevelClient;

    @Override
    public void saveDiscussPost(DiscussPost discussPost) {
        postRepository.save(discussPost);
    }

    @Override
    public void deleteDiscussPost(int id) {
        postRepository.deleteById(id);
    }

    @Override
    public SearchResult searchDiscussPost(String keyword, int current, int limit) throws IOException {
        SearchRequest searchRequest = new SearchRequest("discusspost");//discusspost是索引名，就是表名
        //高亮
        HighlightBuilder highlightBuilder = new HighlightBuilder();
        highlightBuilder.field("title");
        highlightBuilder.field("content");
        highlightBuilder.requireFieldMatch(false);
        highlightBuilder.preTags("<span style='color:red'>");
        highlightBuilder.postTags("</span>");

        //构建搜索条件
        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder()
                .query(QueryBuilders.multiMatchQuery(keyword, "title", "content"))
                .sort(SortBuilders.fieldSort("type").order(SortOrder.DESC))
                .sort(SortBuilders.fieldSort("score").order(SortOrder.DESC))
                .sort(SortBuilders.fieldSort("createTime").order(SortOrder.DESC))
                .from(current)  // 指定从哪条开始查询
                .size(limit)    // 需要查出的总记录条数
                .highlighter(highlightBuilder); //高亮

        searchRequest.source(searchSourceBuilder);
        SearchResponse searchResponse = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);

        List<DiscussPost> list = new ArrayList<>();
        long total = searchResponse.getHits().getTotalHits().value;
        for (SearchHit hit : searchResponse.getHits().getHits()) {
            DiscussPost discussPost = JSONObject.parseObject(hit.getSourceAsString(), DiscussPost.class);

            // 处理高亮显示的结果
            HighlightField titleField = hit.getHighlightFields().get("title");
            if (titleField != null) {
                discussPost.setTitle(titleField.getFragments()[0].toString());
            }
            HighlightField contentField = hit.getHighlightFields().get("content");
            if (contentField != null) {
                discussPost.setContent(contentField.getFragments()[0].toString());
            }
//                System.out.println(discussPost);
            list.add(discussPost);
        }
        return new SearchResult(list, total);
    }
}
```

其中，save 和 delete 在添加帖子和删除帖子时调用，具体的调用方式是：

1. 在添加帖子和删除帖子时，先将事件丢入 Kafka，而不是立即更新 ES；
2. 然后再在消费者中调用 save 和 delete 方法进行具体的删除。



search 方法是核心，其主要完成了：

- 关键词的高亮；
- 根据关键词对 titile 和 content 进行匹配；



Controller 层只需要提供一个请求方法即可：

```java
@Controller
public class SearchController {

    @Autowired
    private ElasticsearchService elasticsearchService;

    @Autowired
    private UserService userService;

    @Autowired
    private LikeService likeService;

    /**
     * 通过 ES 搜索帖子
     * @param keyword 搜索关键词
     * @param page 分页信息
     * @param model
     * @return
     * @throws IOException
     */
    // search?keyword=xxx
    @RequestMapping(path = "/search", method = RequestMethod.GET)
    public String search(String keyword, Page page, Model model) throws IOException {
        // 搜索帖子
        SearchResult result = elasticsearchService.searchDiscussPost(keyword, page.getCurrent() - 1, page.getLimit());

        // 聚合数据
        List<Map<String, Object>> discussPosts = new ArrayList<>();
        if (result != null) {
            for (DiscussPost post : result.getList()) {
                Map<String, Object> map = new HashMap<>();
                map.put("post", post);
                map.put("user", userService.findUserById(post.getUserId()));
                map.put("likeCount", likeService.findEntityLikeCount(CommonConstant.ENTITY_TYPE_POST, post.getId()));
                discussPosts.add(map);
            }
        }
        model.addAttribute("discussPosts", discussPosts);
        model.addAttribute("keyword", keyword);

        // 分页信息
        page.setPath("/search?keyword=" + keyword);
        page.setRows(result == null ? 0 : (int) result.getTotal());

        return "/site/search";
    }
}
```



## 6. 定时任务

因为热榜帖子是每隔一段时间更新一次的，所以很适合用定时任务来完成这个业务。



### 6.1 认识 Quartz

使用 JDK 的 ScheduledExecutorService 和 Spring 的 ThreadPoolTaskScheduler 都可以实现定时任务，但是没有 Quartz 功能强大。

Quartz 可以根据 Cron 表达式自由的自定义定时任务的规则，比如每月每天什么时候执行任务。



Quartz 的核心概念：

- Job：即需要完成的工作，需要实现 org.quartz.job 接口，重写 execute 方法，定时器到时后就会执行。
- Trigger：执行任务的触发器，比如你想每天定时 1 点发送邮件，Trigger 将会设置 1 点执行该任务。
- Scheduler：任务的调度器，会将 Job 和 Trigger 结合，负责基于 Trigger 设定的时间执行 Job



### 6.2 热榜帖子实现

在首页的热帖排行榜中，帖子是根据热度排行的，热度用分数来衡量。

我们每次在对帖子进行添加、评论、加精、点赞时，都会对帖子的分数造成影响，所以在这些地方都需要更新帖子分数。

帖子分数肯定不是实时更新的，我们将需要更新的帖子 id 存入 Redis 中，然后用定时任务去 Redis 中取帖子来更新。因为不管我们对帖子做了多少次更改，最后对每一个帖子都只计算一次分数，**为了避免重复对一个帖子进行更新，所以 key 使用 set 类型**。

分析好业务功能后，开始定义我们的 Job：

```java
/**
 * @desc: 定时任务，定时刷新帖子的分数
 * @author: AruNi_Lu
 * @date: 2022/10/23
 */
public class PostScoreRefreshJob implements Job {

    private static final Logger logger = LoggerFactory.getLogger(PostScoreRefreshJob.class);

    @Autowired
    private RedisTemplate redisTemplate;

    @Autowired
    private DiscussPostService discussPostService;

    @Autowired
    private LikeService likeService;

    @Autowired
    private ElasticsearchService elasticsearchService;

    // 牛客纪元，计算分数时需要
    private static final Date NOWCODER_EPOCH;

    static {
        try {
            NOWCODER_EPOCH = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").parse("2014-08-01 00:00:00");
        } catch (ParseException e) {
            throw new RuntimeException("初始化牛客纪元失败！", e);
        }
    }

    @Override
    public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException {
        String redisKey = RedisKeyUtil.getPostScoreKey();
        // 获取绑定 redisKey 的对象，后面直接通过这个对象来进行有关与 redisKey 相关的操作
        BoundSetOperations operations = redisTemplate.boundSetOps(redisKey);

        if (operations.size() == 0) {
            logger.info("[任务取消] 没有需要刷新分数的帖子");
            return;
        }

        logger.info("[任务开始] 正在刷新帖子分数：" + operations.size());

        // 当 set 中有要刷新的帖子时，依次弹出进行刷新
        while (operations.size() > 0) {
            this.refresh((Integer) operations.pop());
        }

        logger.info("[任务结束] 帖子分数刷新完毕！");
    }

    private void refresh(int postId) {
        DiscussPost post = discussPostService.findDiscussPostById(postId);
        if (post == null) {
            logger.error("该帖子不存在：id = " + postId);
            return;
        }

        // 是否加精
        boolean wonderful = post.getStatus() == 1;
        // 评论数量
        int commentCount = post.getCommentCount();
        // 点赞数量
        long likeCount = likeService.findEntityLikeCount(CommonConstant.ENTITY_TYPE_POST, postId);

        // 计算权重
        double w = (wonderful ? 75 : 0) + commentCount * 10 + likeCount * 2;
        // 分数 = 帖子权重 + 距离天数
        double score = Math.log10(Math.max(w, 1))
                + (post.getCreateTime().getTime() - NOWCODER_EPOCH.getTime()) / (1000 * 3600 * 24);
        // 更新帖子分数
        discussPostService.updateScore(postId, score);

        // 同步 ES 数据
        post.setScore(score);
        elasticsearchService.saveDiscussPost(post);
    }
}
```

接着去配置 Trigger 即可：

```java
/**
 * @desc: Quartz 配置类：配置 -> 数据库 -> 调用
 *      1. 项目启动，该配置被加载，下面的配置会被插入到数据库
 *      2. 接着就可以根据数据库中的配置数据进行调度
 * @author: AruNi_Lu
 * @date: 2022/10/22
 */
@Configuration
public class QuartzConfig {
    /*
    FactoryBean 可简化 Bean 的实例化过程：
        1. 通过 FactoryBean 封装 Bean 的实例化过程
        2. 将 FactoryBean 装配到 Spring 容器
        3. 将 FactoryBean 注入给其他的 Bean
        4. 该 Bean 得到的是 FactoryBean 所管理的对象实例
     */

    // 刷新帖子分数任务
    // 配置 JobDetail，装配到 Spring 容器
    @Bean
    public JobDetailFactoryBean PostScoreRefreshJobDetail() {
        JobDetailFactoryBean factoryBean = new JobDetailFactoryBean();
        factoryBean.setJobClass(PostScoreRefreshJob.class);
        factoryBean.setName("postScoreRefreshJob");
        factoryBean.setGroup("nowcoderGroup");
        factoryBean.setDurability(true);
        factoryBean.setRequestsRecovery(true);
        return factoryBean;
    }

    // 配置 Trigger，将名为 PostScoreRefreshJobDetail 的 Bean 通过 Bean 名字注入进来，直接使用
    @Bean
    public SimpleTriggerFactoryBean PostScoreRefreshTrigger(JobDetail PostScoreRefreshJobDetail) {
        SimpleTriggerFactoryBean factoryBean = new SimpleTriggerFactoryBean();
        factoryBean.setJobDetail(PostScoreRefreshJobDetail);
        factoryBean.setName("postScoreRefreshTrigger");
        factoryBean.setGroup("nowcoderGroup");
        factoryBean.setRepeatInterval(1000 * 60 * 5);    // 5min 执行一次
        factoryBean.setJobDataMap(new JobDataMap());
        return factoryBean;
    }
}
```

然后在对帖子进行添加、评论、加精、点赞时，将对应的 postId 存入 Redis 即可，当 Redis 中该 key 对应的 set 集合不为空时，定时任务会自动从 Redis 中取出帖子来更新。



## 7. 二级缓存优化性能

我们的热榜帖子是使用定时任务更新的，所以更新不会很频繁，而且热榜帖子通常是访问最频繁的。

因此，热榜帖子非常适合再加一个 **本地缓存**。将数据缓存在应用服务器上，性能最好。

因为 Redis 需要网络开销，增加时耗。本地缓存是直接从本地内存中读取，没有网络开销，比较适合数据量较小的缓存。我们只缓存 15 - 20 个热点帖子到本地，因为一般用户只看前两三页。

我们规定了热榜帖子的 orderMode 为 1，所以我们只对 oderMode 为 1，userId 为 0（userId 为 0 表示查询所有帖子，不为 0 表示查询某个 user 的帖子）的查询帖子列表加本地缓存。

除了对帖子列表的查询做本地缓存外，还有查询帖子的总页数也要做，因为加载热榜帖子页面时，也要加载总页数，不能让加载总页数的查询拖了后腿。



### 7.1 认识 Caffeine

本地缓存常用的工具有 Ehcache、Guava、Caffeine 等，我们使用 Caffeine，它是目前性能最好的缓存工具。



> Caffeine 配置说明

如下图：

![img](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210311545410.webp)

注意：

- weakValues 和 softValues 不可以同时使用。
- maximumSize 和 maximumWeight 不可以同时使用。
- expireAfterWrite 和 expireAfterAccess 同事存在时，以 expireAfterWrite 为准。



### 7.2 使用 Caffeine

首先，肯定是在原始的业务层去添加本地缓存，所以定位到 `DiscussPostServiceImpl`。

Caffeine 的核心接口是 Cache，子接口有 LoadingCache（同步加载），AsyncLoadingCache（异步加载）。

Caffeine 内部通过 ConcurrentHashMap 实现，所以使用加载和获取缓存都是通过哈希表的 key-value 方式。



在 DiscussPostServiceImpl 业务层定义缓存，且初始化好：

```java
/**
 * @desc: 帖子相关业务类，使用 Caffeine 本地缓存 热点帖子（定期更新能保证变更不频繁，而访问很频繁）
 * @author: AruNi_Lu
 * @date: 2022/10/8
 */
@Service
public class DiscussPostServiceImpl implements DiscussPostService {

    private static final Logger logger = LoggerFactory.getLogger(DiscussPostService.class);

    @Autowired
    private DiscussPostMapper discussPostMapper;

    @Autowired
    private SensitiveFilter sensitiveFilter;

    @Value("${caffeine.posts.max-size}")
    private int maxSize;

    @Value("${caffeine.posts.expire-seconds}")
    private int expireSeconds;

    // Caffeine 核心接口：Cache， 子接口：LoadingCache，AsyncLoadingCache

    // 帖子列表的缓存，key 为 offset:limit； value 为一页的帖子列表
    private LoadingCache<String, List<DiscussPost>> postListCache;

    // 帖子总数的缓存，key 为 userId（永远都是 0，表示查询所有帖子）; value 为帖子的总行数
    private LoadingCache<Integer, Integer> postRowsCache;

    /**
     * 初始化帖子列表和总数的缓存
     */
    @PostConstruct
    public void init() {
        postListCache = Caffeine.newBuilder()
                .maximumSize(maxSize)
                .expireAfterWrite(expireSeconds, TimeUnit.SECONDS)
                .build(new CacheLoader<String, List<DiscussPost>>() {
                    // load 方法用于查不到本地缓存时，去哪儿取数据，参数是缓存的 key
                    @Override
                    public @Nullable List<DiscussPost> load(@NonNull String key) throws Exception {
                        if (key == null || key.length() == 0) {
                            throw new IllegalArgumentException("参数错误！");
                        }

                        String[] params = key.split(":");
                        if (params == null || params.length != 2) {
                            throw new IllegalArgumentException("参数错误！");
                        }

                        int offset = Integer.parseInt(params[0]);
                        int limit = Integer.parseInt(params[1]);

                        // 中间可以再穿插一个二级缓存（Redis）

                        logger.debug("load post list from DB.");
                        return discussPostMapper.selectDiscussPosts(0, offset, limit, 1);
                    }
                });

        postRowsCache = Caffeine.newBuilder()
                .maximumSize(maxSize)
                .expireAfterWrite(expireSeconds, TimeUnit.SECONDS)
                .build(new CacheLoader<Integer, Integer>() {
                    @Override
                    public @Nullable Integer load(@NonNull Integer key) throws Exception {
                        logger.debug("load post rows from DB.");
                        return discussPostMapper.selectDiscussPostRows(key);
                    }
                });
    }

    @Override
    public List<DiscussPost> findDiscussPosts(int userId, int offset, int limit, int orderMode) {
        // 只缓存热点帖子，userId == 0 表示查询所有帖子（不为 0 表示查询某个 user 的帖子），orderMode 为 1 表示查询热点帖子
        if (userId == 0 && orderMode == 1) {
            // key 的设计：offset 和 limit 能唯一确定一页帖子
            return postListCache.get(offset + ":" + limit);
        }

        logger.debug("load post list from DB.");
        return discussPostMapper.selectDiscussPosts(userId, offset, limit, orderMode);
    }

    @Override
    public int findDiscussPostRows(int userId) {
        if (userId == 0) {
            // key 的设计：userId（永远都是 0，表示查询所有帖子）
            return postRowsCache.get(userId);
        }

        logger.debug("load post rows from DB.");
        return discussPostMapper.selectDiscussPostRows(userId);
    }
    
    // 其他业务方法略
    
}
```

现在当有大量数据时，本地缓存带来的性能提升就很大了。



### 7.3 配合 Redis 做二级缓存

缓存的解决方案一般有三种：

1. 本地内存缓存，如 Caffeine、Ehcache； 适合单机系统，速度最快，但是容量有限，而且重启系统后缓存丢失；
2. 集中式缓存，如 Redis、Memcached； 适合分布式系统，解决了容量、重启丢失缓存等问题，但是当访问量极大时，往往性能不是首要考虑的问题，而是带宽。现象就是 Redis 服务负载不高，但是由于机器网卡带宽跑满，导致数据读取非常慢；
3. 第三种方案就是结合以上 2 种方案的二级缓存应运而生，以内存缓存作为一级缓存、集中式缓存作为二级缓存；



二级缓存方案大致流程如下：

![](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210312114587.png)



> 二级缓存实现

要使用 Redis，首先第一步一定是定义好 key，二级缓存的 key 定义如下：

```java
public class RedisKeyUtil {
    
    // 其他略
    
    private static final String PREFIX_L2CACHE_POSTS = "l2cache:posts";

    private static final String PREFIX_L2CACHE_POST_ROWS = "l2cache:post:rows";
    
    // 二级缓存：帖子列表
    public static String getL2CachePostsKey(int offset, int limit) {
        return PREFIX_L2CACHE_POSTS + SPLIT + offset + SPLIT + limit;
    }

    // 二级缓存：帖子总行数
    public static String getL2CachePostRowsKey() {
        return PREFIX_L2CACHE_POST_ROWS;
    }
    
}
```

这里把二级缓存定义统一的前缀，方便后面做清理操作。



由于我们在更新帖子分数的时候需要清除一级缓存和二级缓存，所以我把 Caffeine 封装成了一个工具类，这样在更新帖子分数时也可以调用本地缓存来进行清理操作。

其实还有一个方案是把本地缓存和 Redis 缓存的过期时间都设置成更新帖子分数的定时任务的时间间隔。这样在下一次更新帖子分数时缓存刚好也失效了，但是这样总感觉有一个约束，我的缓存为什么要依赖于定时任务的时间呢？？

我不管你什么时候执行定时任务更新帖子分数，你也别管我缓存过久过期。只要在你更新帖子分数时我清理掉缓存，**保证数据库和缓存一致** 即可。


抽出来的 CaffeineUtil：

```java
/**
 * @desc: 本地缓存 CaffeineUtil
 * @author: AruNi_Lu
 * @date: 2022/10/31
 */
@Component
public class CaffeineUtil {

    private static final Logger logger = LoggerFactory.getLogger(CaffeineUtil.class);

    @Autowired
    private RedisTemplate redisTemplate;

    @Autowired
    private DiscussPostMapper discussPostMapper;

    @Value("${caffeine.posts.max-size}")
    private int maxSize;

    @Value("${caffeine.posts.expire-seconds}")
    private int expireSeconds;

    // Caffeine 核心接口：Cache， 子接口：LoadingCache，AsyncLoadingCache

    // 帖子列表的缓存，key 为 offset:limit； value 为一页的帖子列表
    public LoadingCache<String, List<DiscussPost>> postListCache;

    // 帖子总数的缓存，key 为 userId（永远都是 0，表示查询所有帖子）; value 为帖子的总行数
    public LoadingCache<Integer, Integer> postRowsCache;

    /**
     * 初始化帖子列表和总数的缓存
     */
    @PostConstruct
    public void init() {
        postListCache = Caffeine.newBuilder()
                .maximumSize(maxSize)
                .expireAfterWrite(expireSeconds, TimeUnit.SECONDS)
                .build(new CacheLoader<String, List<DiscussPost>>() {
                    // load 方法用于查不到本地缓存时，去哪儿取数据，参数是缓存的 key
                    @Override
                    public @Nullable List<DiscussPost> load(@NonNull String key) throws Exception {
                        if (key == null || key.length() == 0) {
                            throw new IllegalArgumentException("参数错误！");
                        }

                        String[] params = key.split(":");
                        if (params == null || params.length != 2) {
                            throw new IllegalArgumentException("参数错误！");
                        }

                        int offset = Integer.parseInt(params[0]);
                        int limit = Integer.parseInt(params[1]);

                        // 中间可以再穿插一个二级缓存（Redis）
                        String redisKey = RedisKeyUtil.getL2CachePostsKey(offset, limit);
                        List<DiscussPost> postList = redisTemplate.opsForList().range(redisKey, 0, -1);
                        if (postList != null && postList.size() != 0) {
                            logger.debug("load post list from Redis.");
                            return postList;
                        }


                        logger.debug("load post list from DB.");
                        postList = discussPostMapper.selectDiscussPosts(0, offset, limit, 1);

                        // 从 DB 查询后初始化 Redis
                        logger.debug("init l2cache posts from DB.");
                        redisTemplate.opsForList().rightPushAll(redisKey, postList);
                        redisTemplate.expire(redisKey, expireSeconds, TimeUnit.SECONDS);

                        return postList;
                    }
                });

        postRowsCache = Caffeine.newBuilder()
                .maximumSize(maxSize)
                .expireAfterWrite(expireSeconds, TimeUnit.SECONDS)
                .build(new CacheLoader<Integer, Integer>() {
                    @Override
                    public @Nullable Integer load(@NonNull Integer key) throws Exception {
                        // 二级缓存
                        String redisKey = RedisKeyUtil.getL2CachePostRowsKey();
                        Integer rows = (Integer) redisTemplate.opsForValue().get(redisKey);
                        if (rows != null) {
                            logger.debug("load post rows from Redis.");
                            return rows;
                        }

                        logger.debug("load post rows from DB.");
                        rows = discussPostMapper.selectDiscussPostRows(key);

                        logger.debug("init l2cache post rows from DB.");
                        redisTemplate.opsForValue().set(redisKey, rows, expireSeconds, TimeUnit.SECONDS);

                        return rows;
                    }
                });
    }

}
```

在 DiscussPostServiceImpl 中直接注入 CaffeineUtil，然后通过它调用 postListCache 和 postRowsCache即可。



在定时任务的工作类 PostScoreRefreshJob 中刷新热帖时，添加上删除缓存的代码，判断是否需要刷新热帖的方法是 execute：

```java
    @Override
    public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException {
        String redisKey = RedisKeyUtil.getPostScoreKey();
        // 获取绑定 redisKey 的对象，后面直接通过这个对象来进行有关与 redisKey 相关的操作
        BoundSetOperations operations = redisTemplate.boundSetOps(redisKey);

        if (operations.size() == 0) {
            logger.info("[任务取消] 没有需要刷新分数的帖子");
            return;
        }

        logger.info("[任务开始] 正在刷新帖子分数：" + operations.size());

        // 当 set 中有要刷新的帖子时，依次弹出进行刷新
        while (operations.size() > 0) {
            this.refresh((Integer) operations.pop());
        }

        // 热帖刷新时，删除一级和二级缓存
        logger.debug("热帖已更新，开始删除一级缓存和二级缓存！");
        caffeineUtil.postListCache.cleanUp();
        caffeineUtil.postRowsCache.cleanUp();
        // 先获取所有关于二级缓存的键，在依次删除即可
        Set keys = redisTemplate.keys("l2cache*");
        for (Object key : keys) {
            redisTemplate.delete(key);
        }

        logger.info("[任务结束] 帖子分数刷新完毕！");
    }
```

至此，二级缓存的功能已完成。



## 8. 如何保证缓存与数据库的数据一致性

如果数据不一致，那么业务应用从缓存中读取的数据就不是最新的数据，这会导致严重的错误。



### 问题一：更新缓存还是淘汰缓存？

我们先来讨论缓存更新的策略问题：即更新缓存时，是直接淘汰cache中的旧数据，还是将更新操作也放在缓存中进行？

**淘汰 cache：**

- 优点：操作简单，无论更新操作是否复杂，直接将缓存中的旧值淘汰
- 缺点：淘汰cache后，下一次查询无法在cache中查到，会有一次cache miss，这时需要重新读取数据库

**更新 cache：**更新chache的意思就是将更新操作也放到缓冲中执行，并不是数据库中的值更新后再将最新值传到缓存。

- 优点：命中率高，直接更新缓存，不会有cache miss的情况
- 缺点：更新 cache 消耗较大

所以选择 **直接淘汰缓存更好**，如果之后需要再次读取这个数据，最多会有一次缓存失败。

而且若有多个线程并发的进行修改操作，若采用更新缓存也会遇到和数据库相同的并发问题，所以还是推荐直接删除缓存。



### 当 Redis 作为读写缓存时

对于读写缓存来说，如果要对数据进行增删改，就需要在缓存中进行，同时还要根据写回策略决定是否同步写回到数据库中。

- **同步直写策略**：写缓存时，也同步写数据库，缓存和数据库中的数据一致
- **异步写回策略**：写缓存时不同步写数据库，等到数据从缓存中淘汰时，再写回数据库。使用这种策略时，如果数据还没有写回数据库，缓存就发生了故障。那么数据库就没有最新数据了。

所以，对于读写缓存来说，要想保证缓存和数据库中的数据一致，就要采用同步直写策略。

不过需要注意的是：如果采用这种策略，**就需要保证同时更新缓存和数据库**。所以，我们 **要在业务应用中使用事务机制**，来保证缓存和数据库的更新具有原子性，两者要不一起更新，要不都不更新，返回错误信息，进行重试。否则就无法实现同步。

当然，在有些场景下，我们对数据一致性的要求可能不是那么高，那么，我们可以使用异步写回策略。



### 当 Redis 作为只读缓存时

对于只读缓存来说：

- 如果有数据新增，会直接写入数据库；
- 有数据删改时，就需要把只读缓存中的数据标记为无效

![在这里插入图片描述](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202211042044505.png)

那么，这个过程中会不会出现数据不一致的情况呢？



对于新增数据，缓存中本来就没有该数据，所以访问时需要从数据库中查询，查到的就是最新的数据，此时不会出现数据不一致的情况。



但对于 **删改数据**，应用既要更新数据库，也要在缓存中删除数据。这两个操作如果无法保证原子性，就会出现 **数据不一致**：

- 假设采用先删除缓存，再更新数据库：如果缓存删除成功，但是数据库更新失败，再访问数据时，缓存中没有数据，就会发生缓存缺失。然后再访问数据库，但是数据库中的值为旧值，应用就访问到旧值了。

  ![在这里插入图片描述](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202211042048427.png)

- 假设采用先更新数据库，再删除缓存：如果数据库更新成功，但是缓存删除失败，那么缓存中的是旧值，这时如果有其他的并发请求来访问数据，会先在缓存中查询，但此时就会读到旧值了。

  ![在这里插入图片描述](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202211042050298.png)



总结：在更新数据库和删除缓存值的过程中，无论这两个操作的执行顺序谁先谁后，只要有一个操作失败了，就会导致客户端读取到旧值



### 如何解决数据不一致？

> 方法一：重试机制

把要删除的缓存值或者要更新的数据库值暂存到消息队列中。当应用没有能够成功的删除缓存值或者是更新数据库值时，可以从消息队列中重新读取这些值，然后再次进行删除或者更新。

如果能够成功删除或者更新，我们就要把这些值从消息队列中去除，以免重复操作。如果重试超过一定次数，还是没有成功，我们就需要向应用层发送报错信息了。

下图显示了先更新数据库，再删除缓存值时，如果缓存删除失败，再次重试后删除成功的情况：

![在这里插入图片描述](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202211042108502.png)

上面说的是在更新数据库和删除缓存值的过程中，其中一个操作失败的情况。



实际上，即使这两个操作第一次执行时都没有失败，当有大量 **并发请求** 时，应用还是有可能读到不一致的数据。

同样使用不同的删除和更新顺序来看：

**先删除缓存，再更新数据库**：

假设线程 A 删除缓存值后，还没有来得及更新数据库（比如说有网络延迟），线程 B 就开始读取数据了，那么这个时候，线程 B 会发现缓存缺失，就只能去数据库读取。这会带来两个问题：

- 线程 B 读取到了旧值；
- 线程 B 是 **在缓存缺失的情况下读取的数据库**，所以 **它还会把旧值写入缓存**，这可能会导致 **其他线程从缓存中读到旧值**。

![在这里插入图片描述](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202211042115669.png)

怎么解决呢？**在线程 A 更新完数据库值以后，我们可以让它先 sleep 一小段时间，再进行一次缓存删除操作。**

- 之所以要加上 sleep 的这段时间，就是为了让线程 B 能够先从数据库读取数据，把缺失的数据写入缓存（此时不是最新的数据），然后线程 A 再进行一次缓存的删除（删除线程 B 写入缓存的数据）。所以线程 A sleep 的时间，就需要大于线程 B 读取数据再写入缓存的时间。

  这个时间怎么确定呢？建议你在业务程序运行的时候，统计下线程读数据和写缓存的操作时间，以此为基础来进行估算。

- 这样一来，其它线程读取数据时，会发现缓存缺失，所以会从数据库中读取最新值。因为这个方案会在第一次删除缓存值后，延迟一段时间再次进行删除，所以我们也把它叫做 “**延迟双删**”。



**先更新数据库值，再删除缓存值**：

- 如果线程 A 删除了数据库中的值，但还没来得及删除缓存值，线程 B 就开始读取数据了，那么此时，线程 B 查询缓存时，发现缓存命中，就会直接从缓存中读取旧值。
- 不过，在这种情况下，如果其他线程并发读缓存的请求不多，那么，就不会有很多请求读取到旧值。而且，线程 A 一般也会很快删除缓存值，这样一来，其他线程再次读取时，就会发生缓存缺失，进而从数据库中读取最新值。所以，这种情况对业务的影响较小。

![在这里插入图片描述](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202211042124305.png)

解决方法也是 “延迟双删”。



### 总结

缓存与数据库的数据一致性主要看有无并发请求和操作顺序：

![在这里插入图片描述](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202211042125775.png)

在大部分业务场景下，我们会把 redis 作为只读缓存使用。针对只读缓存来说，我们既可以先删除缓存值再更新数据库，也可以先更新数据库再删除缓存。建议是，优先使用 **先更新数据库再删除缓存** 的方法，原因主要有两个：

- 先删除缓存值在更新数据库，有可能导致请求因为 **缓存缺失而访问数据库**，**给数据库带来压力**
- 如果业务应用中读取数据库和写缓存的时间不好估算，那么，**延时双删中的等待时间就不好设置**

不过，当使用先更新数据库再删除缓存时，也有个地方需要注意，如果业务层 **要求必须读取一致的数据**，那么，我们就需要在更新数据库时，先 **在 Redis 缓存客户端暂存并发读请求**，等数据库更新完、缓存值删除后，再读取数据，从而保证数据一致性。









## 9. 项目总结

主要的实现的功能和技术栈：

![image-20221026233652193](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210262337123.png)

部署上线时，重要的是性能、安全、可靠性。

![image-20221026233619539](https://run-notes.oss-cn-beijing.aliyuncs.com/notes/202210262337052.png)





## 10. 心得体会

总体来说仿牛客论讨这个项目还是能学到很多东西的，老师讲的也好，通俗易懂。

这是我准备用来找实习的项目，虽然像商城、论坛等已经烂大街了，但是不可否认它们确确实实是好的项目，自己能从中学到东西即可。为了不被面试官发现这是烂大街的项目，可以好好包装以下，换个壳也不是不行。

